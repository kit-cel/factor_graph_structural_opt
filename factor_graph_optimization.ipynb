{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Structural Optimization of Factor Graphs for Symbol Detection via Continuous Clustering and Machine Learning\n",
    "---\n",
    "Lukas Rapp, Luca Schmid, Andrej Rode, and Laurent Schmalen\n",
    "\n",
    "We propose a novel method to optimize the structure of factor graphs for graph-based inference. As an example inference task, we consider symbol detection on linear inter-symbol interference channels.  The factor graph framework has the potential to yield low-complexity symbol detectors. However, the sum-product algorithm on cyclic factor graphs is suboptimal and its performance is highly sensitive to the underlying graph. Therefore, we optimize the structure of the underlying factor graphs in an end-to-end manner using machine learning. For that purpose, we transform the structural optimization into a clustering problem of low-degree factor nodes that incorporates the known channel model into the optimization. Furthermore, we study the combination of this approach with neural belief propagation, yielding near-maximum a posteriori symbol detection performance for specific channels.\n",
    "\n",
    "The full paper [1] is available on arXiv.\n",
    "\n",
    "[1] L. Rapp, L. Schmid, A. Rode, and L. Schmalen, “Structural Optimization of Factor Graphs for Symbol Detection via Continuous Clustering and Machine Learning,” 2023, arXiv:2211.11406. [Online]. Available: https://arxiv.org/abs/2211.11406.\n",
    "\n",
    "For questions, discussions or improvements of this code, feel free to contact us (Lukas Rapp, first.last@kit.edu) or open an issue/create a pull request.\n",
    "\n",
    "---\n",
    "\n",
    "This work has received funding in part from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No. 101001899) and in part from the German Federal Ministry of Education and Research (BMBF) within the project Open6GHub (grant agreement 16KISK010).\n",
    "\n",
    "---\n",
    "\n",
    "Copyright (c) 2022-2023 Lukas Rapp - Communications Engineering Lab (CEL), Karlsruhe Institute of Technology (KIT)\n",
    "\n",
    "<sup> Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "<sup> The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "<sup> THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import external libraries\n",
    "import numpy as np\n",
    "import torch as t\n",
    "device = t.device(\"cuda\") if t.cuda.is_available() else t.device(\"cpu\")\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from dataclasses import dataclass\n",
    "import factor_graph as fg"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T11:03:07.751348Z",
     "end_time": "2023-05-09T11:03:09.257757Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sec. 2.2. Symbol Detection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define impulse responses\n",
    "h_paper = [0.407, 0.1, 0.815, 0.1, 0.407]\n",
    "h_proakis_c = [0.227, 0.460, 0.688, 0.460, 0.277]\n",
    "\n",
    "# Define the channel model as defined in equation (1) in Sec. 2.2\n",
    "class FiniteMemoryChannel:\n",
    "    \"\"\"\n",
    "    Describes the channel model in Sec. 2.2, equation (1), i.e., the cyclic convolution with the finite impulse response h\n",
    "    followed by AWGN.\n",
    "    \"\"\"\n",
    "    def __init__(self, h: t.tensor, snr_db: float, device='cpu', normalize_taps=True):\n",
    "        \"\"\"\n",
    "        @param h: Channel impulse response\n",
    "        @param snr_db: SNR of AWGN\n",
    "        @param device: PyTorch device\n",
    "        @param normalize_taps: Determines if the impulse response is normalized so that its energy is 1.\n",
    "        \"\"\"\n",
    "\n",
    "        self.h = h.to(device)\n",
    "        if normalize_taps:\n",
    "            self.h = self.h / t.sqrt(t.sum(t.abs(self.h) ** 2))\n",
    "\n",
    "        self.memory = t.numel(self.h) - 1\n",
    "        self.snr_lin = 10 ** (snr_db / 10)\n",
    "        self.device = device\n",
    "\n",
    "    def apply(self, x: t.Tensor) -> t.Tensor:\n",
    "        \"\"\"\n",
    "        Applies channel model equation (1) to x\n",
    "        @param x: transmitted symbols\n",
    "        @return: y: received distorted symbols\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculates convolution of x and h: For this, first the convolution of x and h is calculated using F.conv1d\n",
    "        # and the effect of the cyclic conv. is added afterward.\n",
    "        y_not_cyclic = (F.conv1d(x.real[:, None, :], t.flip(self.h, dims=[0])[None, None, :], padding=self.memory)\n",
    "                        + 1.0j * F.conv1d(x.imag[:, None, :], t.flip(self.h, dims=[0])[None, None, :], padding=self.memory))[:, 0, :]\n",
    "\n",
    "        if self.memory > 0:\n",
    "            y = y_not_cyclic[:, :-self.memory]\n",
    "            y[:, :self.memory] += y_not_cyclic[:, -self.memory:]\n",
    "        else:\n",
    "            y = y_not_cyclic\n",
    "\n",
    "        # Add AWGN\n",
    "        y += t.randn(y.shape, dtype=t.cfloat, device=self.device) / np.sqrt(self.snr_lin)\n",
    "        return y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T11:03:12.753348Z",
     "end_time": "2023-05-09T11:03:12.800964Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Equation (4): Factors of the UFG\n",
    "\n",
    "def calc_F_I_potentials_UFG(y: t.Tensor, channel: FiniteMemoryChannel, constellation: t.Tensor) -> tuple[t.Tensor, t.Tensor]:\n",
    "    \"\"\"\n",
    "    Calculates the factors (in log-domain) of the UFG using equation (4) in the paper\n",
    "    @param y: Received distorted symbols (equation (1))\n",
    "    @param channel: FiniteMemoryChannel object representing the channel impulse response and the awgn noise\n",
    "    @param constellation: tensor container a set of constellation points that each VN can take\n",
    "    @return: tuple (F_log, I_log) of tensors\n",
    "    F_log[batch_idx, k, value_idx] = log(F_k(self.constellation[value_idx])) where F_k is the factor in equation (4)\n",
    "    I_log[batch_idx, l, value_idx_1, value_idx_2]\n",
    "        = log(I_l(self.constellation[value_idx_1], self.constellation[value_idx_2])) where I_l is the factor in equation (4)\n",
    "    \"\"\"\n",
    "\n",
    "    h = channel.h\n",
    "\n",
    "    # Auxiliary 1-dim tensor to calculate F and I (q[l] corresponds to q_{l+1} in equation (4) of the paper)\n",
    "    q = t.zeros(len(h), device=h.device)\n",
    "    for l in range(len(h)):\n",
    "        if l == 0:\n",
    "            q[l] = t.sum(h * t.conj(h))\n",
    "        else:\n",
    "            q[l] = t.sum(h[l:] * t.conj(h[:-l]))\n",
    "\n",
    "    # Calculates the correlation between h and y (first term in the exponential) of F_k\n",
    "    # correlation_h_and_y_(not_)cyclic are tensors with indices [batch_idx, k] where \"k\" corresponds to the index of F_k\n",
    "    # To speed up the calculation, we use the convolution of Pytorch resulting in a non-cyclic convolution,\n",
    "    # and add the effect of the cyclic convolution in the next step resulting in \"correlation_h_and_y_cyclic\"\n",
    "    correlation_h_and_y_not_cyclic = (F.conv1d(y.real[:, None, :], channel.h[None, None, :], padding=channel.memory) +\n",
    "                                      1.0j * F.conv1d(y.imag[:, None, :], channel.h[None, None, :], padding=channel.memory))[:, 0, :]\n",
    "    correlation_h_and_y_cyclic = correlation_h_and_y_not_cyclic[:, channel.memory:]\n",
    "    correlation_h_and_y_cyclic[:, -channel.memory:] += correlation_h_and_y_not_cyclic[:, :channel.memory]\n",
    "\n",
    "    # F_potentials as defined in the docstring above\n",
    "    F_potentials = 2 * channel.snr_lin * t.real(\n",
    "        correlation_h_and_y_cyclic[:, :, None] * t.conj(constellation[None, None, :])\n",
    "        - q[0] / 2 * t.abs(constellation[None, None, :]) ** 2\n",
    "    )\n",
    "\n",
    "    # I_potentials as defined in the docstring above\n",
    "    I_potentials = -2 * channel.snr_lin * t.real(\n",
    "        q[1:, None, None] * t.conj(constellation[None, :, None]) * constellation[None, None, :])\n",
    "\n",
    "    return F_potentials, I_potentials\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T11:03:13.281107Z",
     "end_time": "2023-05-09T11:03:13.296759Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sec. 3.1. Factor Node Containers\n",
    "\n",
    "The next cell defines a factor graph class that stores the FN containers used for clustering. The implementation follows Sec. 3.1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The first two data classes are data structures for FNs of degree 1 and 2 in the basic factor graph (UFG) and store lists of FN containers with which the FNs are connected. Similar to the set M_m in equation (7).\n",
    "\n",
    "@dataclass\n",
    "class FNDeg2:\n",
    "    vn_idx_0: int # Index of the first VN with which the FN is connected\n",
    "    vn_idx_1: int # Index of the second VN with which the FN is connected\n",
    "    vn_distance: int # Distance between both VNs with which the FN is connected: vn_distance = |vn_idx_1 - vn_idx_0|\n",
    "\n",
    "    # List of all FN containers with which the FN can be connected:\n",
    "    # Each entry  [a, [b, c]] is a FN container (idx: a) at which the FN can be connected at slot b and c\n",
    "    connectable_fn_container: list[tuple[int, tuple[int, int]]]\n",
    "\n",
    "@dataclass\n",
    "class FNDeg1:\n",
    "    vn_idx_0: int # Index of the VN with which the FN is connected\n",
    "\n",
    "    # List of all FN containers with which the FN can be connected\n",
    "    # Each entry  [a, b] is an FN container (idx: a) at which the FN can be connected at slot b\n",
    "    connectable_fn_container: list[tuple[int, int]]\n",
    "\n",
    "\n",
    "class ContainerFactorGraph(fg.FactorGraph):\n",
    "    \"\"\"\n",
    "    Extends the general factor graph class by containers of a fixed degree in which FNs of degree 1 and 2 of the basic factor graph for symbol detection\n",
    "    (here Ungerboeck-based factor graph (UFG)) can be clustered\n",
    "    \"\"\"\n",
    "\n",
    "    # List of all FN containers: Each entry is a tuple containing the indices of the VNs with which the FN container is connected\n",
    "    fn_container_list = None\n",
    "\n",
    "    def __init__(self, block_len: int, number_constellation_points: int, max_span_fn_container: int, channel_length: int,\n",
    "                 fn_container_degree: int, device: t.device):\n",
    "        \"\"\"\n",
    "        @param block_len: Length of the symbol sequence for symbol detection (\"K\" in the paper)\n",
    "        @param number_constellation_points: Number of different constellation points each symbol can take\n",
    "        @param max_span_fn_container: Maximal span of the fn containers as defined in the paper Sec 3.1\n",
    "        @param channel_length: Length of the channel impulse response\n",
    "        @param fn_container_degree: degree of all fn containers\n",
    "        @param device: PyTorch device\n",
    "        \"\"\"\n",
    "        self.fn_container_degree = fn_container_degree\n",
    "\n",
    "        if fn_container_degree == 3:\n",
    "            self.fn_container_list = self._create_degree_3_fn_containers(block_len, max_span_fn_container)\n",
    "        elif fn_container_degree == 4:\n",
    "            self.fn_container_list = self._create_degree_4_fn_containers(block_len, max_span_fn_container)\n",
    "        else:\n",
    "            raise NotImplementedError(f'degree {fn_container_degree} fn container not implemented.')\n",
    "\n",
    "        self.fn_degree_2_list = self._create_fns_degree_2(channel_length, block_len)\n",
    "        self.fn_degree_1_list = self._create_fns_degree_1(block_len)\n",
    "\n",
    "        # Using the fn_lists, calculate the parameters which parameterize this FG and create a general FG with them.\n",
    "        biadjacency, fn_start_slots = self._create_fg_parameters(block_len)\n",
    "        super().__init__(biadjacency, number_constellation_points, device, False, fn_start_slots)\n",
    "\n",
    "    def _create_degree_4_fn_containers(self, block_len: int, max_span_fn_container: int) -> list[tuple]:\n",
    "        \"\"\"\n",
    "        Create all the degree 4 fn containers with maximal span max_span_fn_container that can be connected with the VNs\n",
    "        representing the symbols\n",
    "        @param block_len: Length of the symbol sequence for symbol detection (\"K\" in the paper)\n",
    "        @param max_span_fn_container: Maximal span of the fn containers as defined in the paper Sec 3.1\n",
    "        @return: List of FN containers: Each entry is a tuple containing the indices of the VNs with which the FN container is connected\n",
    "        \"\"\"\n",
    "\n",
    "        fn_container_list = []\n",
    "        for vn_idx_0 in range(block_len):\n",
    "            for local_idx_1 in range(1, max_span_fn_container - 2):\n",
    "                for local_idx_2 in range(local_idx_1 + 1, max_span_fn_container - 1):\n",
    "                    for local_idx_3 in range(local_idx_2 + 1, max_span_fn_container):\n",
    "                        vn_idx_1 = (vn_idx_0 + local_idx_1) % block_len\n",
    "                        vn_idx_2 = (vn_idx_0 + local_idx_2) % block_len\n",
    "                        vn_idx_3 = (vn_idx_0 + local_idx_3) % block_len\n",
    "\n",
    "                        fn_container_list.append((vn_idx_0, vn_idx_1, vn_idx_2, vn_idx_3))\n",
    "\n",
    "        return fn_container_list\n",
    "\n",
    "    def _create_degree_3_fn_containers(self, block_len: int, max_span_fn_container: int) -> list[tuple]:\n",
    "        \"\"\"\n",
    "        Create all the degree 3 fn containers with maximal span max_span_fn_container that can be connected with the VNs\n",
    "        (#block_len) representing the symbols\n",
    "        @param block_len: Length of the symbol sequence for symbol detection (\"K\" in the paper)\n",
    "        @param max_span_fn_container: Maximal span of the fn containers as defined in the paper Sec 3.1\n",
    "        @return: List of FN containers: Each entry is a tuple containing the indices of the VNs with which the FN container is connected\n",
    "        \"\"\"\n",
    "\n",
    "        fn_container_list = []\n",
    "        for vn_idx_0 in range(block_len):\n",
    "            for local_idx_1 in range(1, max_span_fn_container - 1):\n",
    "                for local_idx_2 in range(local_idx_1 + 1, max_span_fn_container):\n",
    "                    vn_idx_1 = (vn_idx_0 + local_idx_1) % block_len\n",
    "                    vn_idx_2 = (vn_idx_0 + local_idx_2) % block_len\n",
    "\n",
    "                    fn_container_list.append((vn_idx_0, vn_idx_1, vn_idx_2))\n",
    "\n",
    "        return fn_container_list\n",
    "\n",
    "    def _create_fns_degree_2(self, channel_length: int, block_len: int) -> list[FNDeg2]:\n",
    "        \"\"\"\n",
    "        Create all FNs of degree 2 of the UFG that can be connected with the VNs (#block_len) representing the symbols\n",
    "        @param channel_length: Length of the channel impulse response\n",
    "        @param block_len: Length of the symbol sequence for symbol detection (\"K\" in the paper)\n",
    "        @return: List of FNDeg2 of all FNs of degree 2 which can be connected with the VNs of the FG\n",
    "        \"\"\"\n",
    "\n",
    "        fn_deg2_list = []\n",
    "        for vn_distance in range(1, channel_length):\n",
    "            for vn_idx_0 in range(block_len):\n",
    "                vn_idx_1 = (vn_idx_0 + vn_distance) % block_len\n",
    "\n",
    "                local_fn_degX_list = []\n",
    "                for slot_1 in range(1, self.fn_container_degree):\n",
    "                    for slot_0 in range(slot_1):\n",
    "                        local_fn_degX_list.extend([(fn_degX_Idx, (slot_0, slot_1))\n",
    "                            for fn_degX_Idx, fn_degX in enumerate(self.fn_container_list)\n",
    "                            if (vn_idx_0 == fn_degX[slot_0] and vn_idx_1 == fn_degX[slot_1])])\n",
    "\n",
    "                fn_deg2_list.append(FNDeg2(vn_idx_0, vn_idx_1, vn_distance, local_fn_degX_list))\n",
    "\n",
    "        return fn_deg2_list\n",
    "\n",
    "    def _create_fns_degree_1(self, block_len: int) -> list[FNDeg1]:\n",
    "        \"\"\"\n",
    "        Create all FNs of degree 1 of the UFG that can be connected with the VNs (#block_len) representing the symbols\n",
    "        @param block_len: Length of the symbol sequence for symbol detection (\"K\" in the paper)\n",
    "        @return: List of FNDeg1 of all FNs of degree 1 which can be connected with the VNs of the FG\n",
    "        \"\"\"\n",
    "\n",
    "        fn_deg1_list = []\n",
    "        for vn_idx_0 in range(block_len):\n",
    "            local_fn_degX_list = []\n",
    "            for slot_fn_degX in range(self.fn_container_degree):\n",
    "                local_fn_degX_list.extend([(fn_degX_idx, slot_fn_degX) for fn_degX_idx, fn_degX in enumerate(self.fn_container_list) if vn_idx_0 == fn_degX[slot_fn_degX]])\n",
    "            fn_deg1_list.append(FNDeg1(vn_idx_0, local_fn_degX_list))\n",
    "        return fn_deg1_list\n",
    "\n",
    "    def _create_fg_parameters(self, block_len) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Calculate parameters needed to create FG\n",
    "        @param block_len: Length of the symbol sequence for symbol detection (\"K\" in the paper)\n",
    "        @return: tuple (biadjacency, fn_start_slots)\n",
    "        \"\"\"\n",
    "\n",
    "        biadjacency = t.zeros(block_len, len(self.fn_container_list), dtype=t.long)\n",
    "        fn_start_slots = t.zeros(len(self.fn_container_list), dtype=t.long)\n",
    "        for idx, vn_list in enumerate(self.fn_container_list):\n",
    "            for i in range(self.fn_container_degree - 1):\n",
    "                if vn_list[i + 1] < vn_list[i]:\n",
    "                    fn_start_slots[idx] = i + 1\n",
    "                    break\n",
    "\n",
    "            for vn_idx in vn_list:\n",
    "                biadjacency[vn_idx, idx] = 1\n",
    "\n",
    "        return biadjacency, fn_start_slots"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T11:03:14.215770Z",
     "end_time": "2023-05-09T11:03:14.230801Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sec. 3.2 and 3.3. Continuous Clustering of FNs\n",
    "\n",
    "The following class ClusteredFactorGraph implements continuous clustering and calculates the factors of the clustered FN containers (equation (6)).\n",
    "These factors are later used in the SPA in the FactorGraph class.\n",
    "\n",
    "Since the practical implementation of \"factors\" differs from the mathematical definition, we will give a brief overview of how factors are handled in the FactorGraph class:\n",
    "The FactorGraph class applies the SPA in the log-domain and therefore, also the local function of an FN is stored in log-domain which we call the \"potential\" of an FN in the following.\n",
    "The potential $t$ of a FN  $f(x_1, ..., x_n)$ is an n-dimensional PyTorch tensor with dimension [len(self.constellation, ..., len(self.constellation)] whose entries store all values which $f(\\cdot)$ can take:\n",
    "$$\n",
    "t(i_1, ..., i_n) = \\log(f(\\text{self.constellation}[i_1], ..., \\text{self.constellation}[i_n]), \\qquad \\text{for all $(i_1, ..., i_n) \\in |\\text{len(self.constellation)}|^n$}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ClusteredFactorGraph(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the continuous clustering of the FNs of the basis factor graph and manages the weights of the NBP\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, block_len: int, constellation: t.Tensor, max_span_fn_container: int, channel_length: int,\n",
    "                 fn_container_degree: int, bp_iterations: int, device: t.device):\n",
    "        \"\"\"\n",
    "        @param block_len: Length of the symbol sequence for symbol detection (\"K\" in the paper)\n",
    "        @param constellation: tensor container a set of constellation points that each VN can take\n",
    "        @param max_span_fn_container: Maximal span of the fn containers as defined in the paper Sec 3.1\n",
    "        @param channel_length: Length of the channel impulse response\n",
    "        @param fn_container_degree: degree of all fn containers\n",
    "        @param bp_iterations: Number of belief propagation that will be executed\n",
    "        @param device: PyTorch device\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.bp_iterations = bp_iterations\n",
    "\n",
    "        self.constellation = constellation\n",
    "        self.block_len = block_len\n",
    "        self.device = device\n",
    "\n",
    "        self.graph = ContainerFactorGraph(block_len, len(constellation), max_span_fn_container,\n",
    "                                          channel_length, fn_container_degree, device)\n",
    "\n",
    "        # Weights for clustering of FNs of degree 1 (Called \"beta_ij\" in the paper. See Sec. 3.3 of the paper equation (8).)\n",
    "        self.clustering_weights_fn_deg_1 = []\n",
    "        for idx, fn in enumerate(self.graph.fn_degree_1_list):\n",
    "            weights = nn.Parameter(t.randn(len(fn.connectable_fn_container), requires_grad=True, device=device))\n",
    "            self.register_parameter(f'deg1_{idx}', weights)\n",
    "            self.clustering_weights_fn_deg_1.append(weights)\n",
    "\n",
    "        # Weights for clustering of FNs of degree 1 (Called \"beta_ij\" in the paper. See Sec. 3.3 of the paper equation (8).)\n",
    "        self.beta_fn_deg_2 = []\n",
    "        for idx, fn in enumerate(self.graph.fn_degree_2_list):\n",
    "            weights = nn.Parameter(t.randn(len(fn.connectable_fn_container), requires_grad=True, device=device))\n",
    "            self.register_parameter(f'deg2_{idx}', weights)\n",
    "            self.beta_fn_deg_2.append(weights)\n",
    "\n",
    "        # Weights for NBP\n",
    "        self.weights_vn_incoming_messages = \\\n",
    "            nn.Parameter(t.ones((bp_iterations, self.graph.vn_connections.shape[0], self.graph.vn_degree_max), device=device))\n",
    "        self.weights_fn_incoming_messages = \\\n",
    "            nn.Parameter(t.ones((bp_iterations, self.graph.fn_connections.shape[0], self.graph.fn_degree_max), device=device))\n",
    "\n",
    "        self.graph.set_nbp_weights(self.weights_vn_incoming_messages, self.weights_fn_incoming_messages)\n",
    "\n",
    "    def forward(self, y: t.Tensor, channel: FiniteMemoryChannel) -> t.Tensor:\n",
    "        \"\"\"\n",
    "        Executes the sum-product algorithm on the factor graph and returns the log beliefs of the VNs\n",
    "        @param y: Received symbol sequence with noise (called y in the paper, see equation (1)).\n",
    "        @param channel: Channel with which the transmitted symbols are distorted.\n",
    "        @return:Logarithmic belief for each VN: tensor with indices (batch, vn_idx, message_values)\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = y.shape[0]\n",
    "\n",
    "        # Calculates the factors F_k(x_k), I_l(x_k, x_{k+l}) of equation (4) in the paper in log-domain where\n",
    "        # F_log[batch_idx, k, value_idx] = log(F_k(self.constellation[value_idx])) and\n",
    "        # I_log[batch_idx, l, value_idx_1, value_idx_2] = log(I_l(self.constellation[value_idx_1], self.constellation[value_idx_2]))\n",
    "        F_log, I_log = calc_F_I_potentials_UFG(y, channel, self.constellation)\n",
    "\n",
    "        # Calculate the potentials of the clustered factor graph\n",
    "        log_potentials_clustered_fg = t.zeros((len(self.graph.fn_container_list), batch_size,) +\n",
    "            (len(self.constellation),) * self.graph.fn_container_degree, device=self.device)\n",
    "\n",
    "        # Implements equation (7) for FNs of degree 2 (the variables i, j, and m correspond to the variables in equation (7)):\n",
    "        # For all FNs f of degree 2 in the graph, the loop iterates over all FN containers that are connected with the FN f\n",
    "        # and adds the potential of f weighted by the corresponding clustering coefficient to the potential of the FN container.\n",
    "        for i, fn_i_data in enumerate(self.graph.fn_degree_2_list):\n",
    "            alpha_i = F.softmax(self.beta_fn_deg_2[i], 0)\n",
    "\n",
    "            # Iterate over all FN containers with which the FN_i can be connected.\n",
    "            for j in range(len(fn_i_data.connectable_fn_container)):\n",
    "                (m, (fn_slot_vn_1, fn_slot_vn_2)) = fn_i_data.connectable_fn_container[j]\n",
    "\n",
    "                # Adds the potential of the current FN of degree 2 (I_log) weighted by alpha_fn_deg_2\n",
    "                # (Since the factors of the FNs are in log-domain the multiplication in equation (7) becomes an addition\n",
    "                # and the exponentiation with alpha becomes a multiplication with alpha)\n",
    "                log_potentials_clustered_fg[m] += \\\n",
    "                    alpha_i[j] * I_log[\n",
    "                        [fn_i_data.vn_distance - 1, None]\n",
    "                        + fg.create_indexing(self.graph.fn_container_degree, fn_slot_vn_1, fn_slot_vn_2)\n",
    "                        ]\n",
    "\n",
    "        # Implements equation (7) for FNs of degree 1 (similar to the loop above for FNs of degree 2)\n",
    "        for fn_deg_1_idx, fn_deg_1_data in enumerate(self.graph.fn_degree_1_list):\n",
    "            alpha_fn_deg_1 = F.softmax(self.clustering_weights_fn_deg_1[fn_deg_1_idx], 0)\n",
    "\n",
    "            # Adds the potential of the current FN of degree 1 (F_log) weighted by alpha_fn_deg_1\n",
    "            # (Since the factors of the FNs are in log-domain the multiplication in equation (7) becomes an addition\n",
    "            # and the exponentiation with alpha becomes a multiplication with alpha)\n",
    "            for j in range(len(fn_deg_1_data.connectable_fn_container)):\n",
    "                (m, fn_slot_vn_1) = fn_deg_1_data.connectable_fn_container[j]\n",
    "\n",
    "                log_potentials_clustered_fg[m] += \\\n",
    "                    alpha_fn_deg_1[j] * F_log[\n",
    "                        [slice(None), fn_deg_1_idx] + fg.create_indexing(self.graph.fn_container_degree, fn_slot_vn_1)\n",
    "                        ]\n",
    "\n",
    "        self.graph.load_potentials(log_potentials_clustered_fg, batch_size)\n",
    "        return self.graph.sum_product_algorithm(self.bp_iterations, True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T11:03:15.127424Z",
     "end_time": "2023-05-09T11:03:15.143066Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sec. 3.3: Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def simulate_detection(batch_size: int, block_len: int, constellation: t.Tensor,\n",
    "                       channel: FiniteMemoryChannel, clustered_factor_graph: ClusteredFactorGraph,\n",
    "                       device: t.device) -> tuple[t.tensor, t.tensor]:\n",
    "    \"\"\"\n",
    "    This function distorts random bits using the channel model, applies symbol detection on them, and calculates the resulting BER and loss.\n",
    "    @param batch_size:\n",
    "    @param block_len: Length of the symbol sequence for symbol detection (\"K\" in the paper)\n",
    "    @param constellation: tensor container a set of constellation points that each VN can take\n",
    "    @param channel: Channel with which the transmitted symbols are distorted.\n",
    "    :param clustered_factor_graph: Clustered factor graph model which is used for symbol detection.\n",
    "    :param device: PyTorch device\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Section 2.2\n",
    "    tx_bits = t.randint(2, (batch_size, block_len))\n",
    "    x = constellation[tx_bits]\n",
    "    # Apply channel model equation (1)\n",
    "    y = channel.apply(x)\n",
    "\n",
    "    # Approximates marginalization of equation (2) by sum-product algorithm on factor graph\n",
    "    x_beliefs = clustered_factor_graph(y, channel)\n",
    "    x_hat = t.argmax(x_beliefs, 2)\n",
    "\n",
    "    # Transforms beliefs of sum-product algo. into probabilities.\n",
    "    x_probs = t.sigmoid(-x_beliefs[..., 0]) #1 / (1 + t.exp(tx_beliefs[..., 0]))\n",
    "\n",
    "    # Calculates soft-ber (loss) and ber\n",
    "    loss = t.sum(t.where(tx_bits.to(device) == 1, 1 - x_probs, x_probs)) / batch_size\n",
    "    ber = t.count_nonzero(x_hat.cpu() != tx_bits) / tx_bits.numel()\n",
    "\n",
    "    return ber, loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T11:03:16.089011Z",
     "end_time": "2023-05-09T11:03:16.151713Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Simulation parameters\n",
    "block_len = 30\n",
    "spa_iterations = 10\n",
    "snr_db = 10\n",
    "constellation = t.tensor([1, -1], dtype=t.cfloat, device=device)\n",
    "channel_taps = t.tensor(h_paper)\n",
    "max_span_fn_container = len(channel_taps)\n",
    "fn_container_degree = 3\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 10000\n",
    "batches = 60000\n",
    "\n",
    "channel = FiniteMemoryChannel(channel_taps, snr_db, device)\n",
    "model = ClusteredFactorGraph(block_len, constellation, max_span_fn_container, len(channel_taps), fn_container_degree, spa_iterations, device)\n",
    "optimizer = t.optim.Adam(model.parameters(), learning_rate)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for batch_idx in range(batches):\n",
    "    ber, loss = simulate_detection(batch_size, block_len, constellation, channel, model, device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'BER: {ber.item()} [{batch_idx+1}/{batches}]')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T11:03:30.590462Z",
     "end_time": "2023-05-09T11:03:37.822191Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "snr_list = np.arange(0, 13, 1)\n",
    "ber_list = np.zeros(snr_list.shape[0], dtype=float)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "model.eval()\n",
    "with t.no_grad():\n",
    "    for snr_idx, snr_db in enumerate(snr_list):\n",
    "        channel = FiniteMemoryChannel(channel_taps, snr_db, device)\n",
    "\n",
    "        ber_list[snr_idx], _ = simulate_detection(batch_size, block_len, constellation, channel, model, device)\n",
    "\n",
    "print(f\" Evaluation for Eb/N0: {snr_list}\\nBER: {ber_list}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T11:03:38.588295Z",
     "end_time": "2023-05-09T11:03:40.019942Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
